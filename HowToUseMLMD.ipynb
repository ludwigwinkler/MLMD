{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The repository is structured through the PyTorch Lightning (PL) framework which takes care of all the boiler plate code.\n",
    "\n",
    "You can check out a quick start at the [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) with more details in the 'Core API' section.\n",
    "\n",
    "PL has three main ingredients:\n",
    "- LightningModule\n",
    "    Defines the model and defines the all the steps of the training pipeline: training + backprop step, validation step, test step, before training routine etc etc\n",
    "- LightningDataModule\n",
    "    Defines the download/get data step, the data preparation step (normalize etc) and provides train, validation and test dataloaders functions which return the required data loaders\n",
    "- Trainer\n",
    "    Combines the LightningModule with the LightningDataModule and runs the whole training loop while taking care of GPUs, logging, callbacks. It has a fit function for training the LightningModule to the LightningDataModule and a test function which automatically sets the LightningModule into inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from MLMD.src.MD_DataUtils import load_dm_data\n",
    "from src.MD_HyperparameterParser import Interpolation_HParamParser\n",
    "from src.MD_PLModules import Interpolator\n",
    "from pytorch_lightning import Trainer, seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we set the hyperparameter parser which creates an ArgumentParser (this can improved by using [module specific hyperparameters](https://pytorch-lightning.readthedocs.io/en/stable/common/hyperparameters.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "hparams = Interpolation_HParamParser(logger=0,\n",
    "                                     plot=0,\n",
    "                                     show=0,\n",
    "                                     load_weights=0,\n",
    "                                     save_weights=0,\n",
    "                                     fast_dev_run=0,\n",
    "                                     project='vibrationalspectra',\n",
    "                                     model='bi_lstm',\n",
    "                                     num_layers=5,\n",
    "                                     num_hidden_multiplier=10,\n",
    "                                     criterion='MAE',\n",
    "                                     interpolation=True,\n",
    "                                     interpolation_mode='adiabatic',\n",
    "                                     integration_mode='diffeq',\n",
    "                                     diffeq_output_scaling=1,\n",
    "                                     dataset=['malonaldehyde_dft.npz', 'benzene_dft.npz', 'ethanol_dft.npz', 'toluene_dft.npz', 'naphthalene_dft.npz', 'salicylic_dft.npz', 'paracetamol_dft.npz',\n",
    "                                              'aspirin_dft.npz', 'keto_100K_0.2fs.npz', 'keto_300K_0.2fs.npz', 'keto_500K_0.2fs.npz'][0],\n",
    "                                     input_length=1,\n",
    "                                     output_length=20,\n",
    "                                     batch_size=49,\n",
    "                                     auto_scale_batch_size=False,\n",
    "                                     optim='adam',\n",
    "                                     lr=1e-3,\n",
    "                                     train_traj_repetition=1,\n",
    "                                     max_epochs=2,\n",
    "                                     limit_train_batches=25,\n",
    "                                     limit_val_batches=25)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After having parsed the hyperparameters, we're ready to load the data.\n",
    "Every dataset is encapsulated in a LightningDataModule, while the datasets themselves are contained in BiDirectional_DataSets which take care of fetching the correct initial and final conditions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malonaldehyde_dft.npz: [Num Trajectory, Time Steps, Features ]=torch.Size([1, 993236, 54]) features\n"
     ]
    }
   ],
   "source": [
    "dm = load_dm_data(hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we initialize the Interpolation module from the hyperparameters and set the required output moments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = Interpolator(**vars(hparams))\n",
    "model.model.set_diffeq_output_scaling_statistics(dm.dy_mu, dm.dy_std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here come two callbacks for early stopping and checkpointing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "early_stop_callback = EarlyStopping(monitor='Val/Epoch' + hparams.criterion, mode='min', patience=3, min_delta=0.0005, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we initialize the Trainer-Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1791: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer.from_argparse_args(hparams,\n",
    "                                         # min_steps=1000,\n",
    "                                         # max_steps=50,\n",
    "                                         callbacks=[early_stop_callback] if hparams.save_weights else [early_stop_callback],\n",
    "                                         val_check_interval=1.,\n",
    "                                         gpus=1 if torch.cuda.is_available() else None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | MD_BiDirectional_LSTM | 8.3 M \n",
      "------------------------------------------------\n",
      "8.3 M     Trainable params\n",
      "108       Non-trainable params\n",
      "8.3 M     Total params\n",
      "33.312    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6347b116a88c4d4dbf90888d7aa25620"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:230: UserWarning: You called `self.log('Val/t', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "852c5e5996884ff185ad050ad3193d8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:230: UserWarning: You called `self.log('Train/t', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/Users/ludwigwinkler/opt/anaconda3/envs/phd39/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:658: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, datamodule=dm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}